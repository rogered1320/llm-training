import os

from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain_core.runnables import RunnablePassthrough, RunnableBranch, RunnableLambda
from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableLambda, RunnableMap
from langchain_core.prompts import ChatPromptTemplate
from prompts import chatbot_prompt_text, q_trans_prompt_text
    
pinecone_api_key = os.environ.get("PINECONE_API_KEY")


def run_qa_chatbot(question: str):
    """Runs the chatbot with the given question, using a rewriter to transform the question,
    a retriever to find relevant documents, and an LLM to generate the answer.
    Returns the answer generated by the LLM."""
    openai_llm = get_llm()
    rewrite_chain = get_rewriter_chain(openai_llm)
    retriever = get_retriever(k=3, score_threshold=0.7)
    retriever_chain = get_retriever_chain(retriever)
    
    prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", chatbot_prompt_text),
            ("human", "{question}"),
        ]
    )
    
    call_llm = prompt_template | openai_llm
    
    # stop_step is a RunnableLambda that returns a default message when invoked.
    stop_step = RunnableLambda(stop_step_fn)
    full_chain = get_full_chain(rewrite_chain, retriever_chain, call_llm, stop_step)
    
    res = full_chain.invoke(question)
    
    for key, value in res.items():
        print(f"{key}: {value}\n\n")
    return res["result"].content


def get_full_chain(rewrite_chain, retriever_chain, call_llm, stop_step):
    """
    Returns a full chain that combines the rewriter, retriever,
    and LLM to answer the question.
    """

    return (
        RunnableMap({
            "map_log": RunnableLambda(lambda x: print("Rewritten question:")),
            "rag_question": rewrite_chain,
            "question": RunnablePassthrough(),
            "original_question": RunnablePassthrough(),  # Passthrough to keep the original question
        })
        .assign(
            log = RunnableLambda(lambda x: print("Listo", x))
        )
        .assign(
            data=RunnableLambda(lambda x: x["rag_question"].content) | retriever_chain,
        )
        .assign(
            result=RunnableBranch(
                (lambda x: not is_context_valid(x["data"]["context"]), stop_step),  # if context is empty -> stop
                # If context is valid, call the LLM with the context and question
                {"context": RunnableLambda(lambda x: x["data"]["context"]),
                 "question": RunnableLambda(lambda x: x["original_question"])} | call_llm,
            )
        )
    )


def get_llm():
    """
    Returns an instance of the OpenAI LLM with the specified model and parameters.
    """
    return ChatOpenAI(
        model="gpt-4.1-nano",
        api_key=os.getenv("OPENAI_API_KEY"),
        temperature=0.7,
        verbose=True
    )


def get_retriever(k: int = 3, score_threshold: float = 0.7):
    """
    Returns a retriever that uses Pinecone to retrieve relevant documents based on the question.
    """
    pc = Pinecone(api_key=pinecone_api_key)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_store = PineconeVectorStore(index=pc.Index("rag-class"), embedding=embeddings)
    return vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": k, "score_threshold": score_threshold},
    )


def format_docs(docs):
    """ Formats the retrieved documents into a string representation."""
    if not docs or len(docs) == 0:
        return None
    return "\n\n".join(str({"page_content": doc.page_content, "url": doc.metadata["source_url"]}) for doc in docs)


def is_context_valid(context):
    """
    Validates that the context is not empty."""
    if not context or len(context) == 0:
        print("Context is empty or invalid.")
        return False
    print("Context is valid.", len(context))
    return True


def get_retriever_chain(retriever):
    """
    Returns a retriever chain that formats the retrieved documents.
    """
    return {
        "context": retriever | format_docs,
        "question": RunnablePassthrough(),
    }


def get_rewriter_chain(openai_llm):
    """
    Returns a chain that rewrites the question to make it suitable for retrieval.
    """
    query_trans_prompt_template = ChatPromptTemplate.from_template(q_trans_prompt_text)
    return query_trans_prompt_template | openai_llm


def stop_step_fn(ctx):
    """
    Returns a default message when invoked.
    """
    return AIMessage(content="I didn't receive enough information to answer your question.")


if __name__ == "__main__":
    # For testing purposes, we can run the chatbot with a relevant question
    question = "Who is the person in charge of the company in Peru?"
    response = run_qa_chatbot(question)
    print("Response:", response)
