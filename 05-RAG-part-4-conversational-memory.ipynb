{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aef709d",
   "metadata": {},
   "source": [
    "# Conversational Memory for LangChain\n",
    "\n",
    "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
    "\n",
    "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058bc10",
   "metadata": {},
   "source": [
    "> ⚠️ **Important**\n",
    "\n",
    "Although it is currently recommended to use the **LangGraph** library to implement chats with memory — since it allows you to efficiently work with complex workflows (for example: 🗓️ saving notes to Google Calendar, 🗄️ querying relational databases, and 🤖 using MCPs) — in this notebook we will use **LangChain** to explain the basic concepts of conversational memory.\n",
    "\n",
    "This will make it easier to understand before moving on to more advanced tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30db397",
   "metadata": {},
   "source": [
    "## LangChain's Memory Types\n",
    "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
    "\n",
    "Throughout the notebook we will be referring to these older memory types and then rewriting them using the recommended RunnableWithMessageHistory class. We will learn about:\n",
    "\n",
    "- **ConversationBufferMemory:** the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
    "- **ConversationBufferWindowMemory:** similar to ConversationBufferMemory, but only keeps track of the last k messages.\n",
    "- **ConversationSummaryMemory:** rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
    "- **ConversationSummaryBufferMemory:** merges the ConversationSummaryMemory and ConversationTokenBufferMemory types.\n",
    "\n",
    "We'll work through each of these memory types in turn, and rewrite each one using the RunnableWithMessageHistory class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f685b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"llm-training-05-rag-p4\"\n",
    "\n",
    "llm =  ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.7,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71822e99",
   "metadata": {},
   "source": [
    "### 1. ConversationBufferMemory with RunnableWithMessageHistory\n",
    "When implementing `unnableWithMessageHistory` we will use LangChain Expression Language **(LCEL)** and for this we need to define our prompt template and LLM components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49f60f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n",
    "pipeline = prompt_template | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851b099",
   "metadata": {},
   "source": [
    "Our `RunnableWithMessageHistory` requires our pipeline to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a503cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f8f4f",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie history) and which to use for the user's query (ie query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a204af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's great! Working with BTS must be an exciting experience, whether you're in management, production, marketing, or another area. How can I assist you related to your work with BTS?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 24, 'total_tokens': 61, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_f12167b370', 'id': 'chatcmpl-C46WvG3JCUSGSanG3RamftuqFH5Ai', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--19c8ef84-3bd2-47c8-a409-edf1fc9c2997-0', usage_metadata={'input_tokens': 24, 'output_tokens': 37, 'total_tokens': 61, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pipeline with history\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I work in BTS\"},\n",
    "    config={\"session_id\": \"id_124\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5929a78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='where is the library?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Could you please specify which library you're referring to or provide your current location? That way, I can help you find the nearest library.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 25, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C46WDUq4t5FiWLxNHG4Rd43UCPlcU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1ad1b5c0-223b-443f-b4b1-7035ccf4cf5e-0', usage_metadata={'input_tokens': 25, 'output_tokens': 27, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='I work in BTS', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Thank you for the clarification. If you're referring to BTS in Seoul, South Korea, there are a few notable locations related to BTS:\\n\\n1. **HYBE Corporation Headquarters (formerly Big Hit Entertainment)** – Located in Seoul, this is where the management and production of BTS are based.\\n2. **BTS Pop-Up Stores and Exhibitions** – Sometimes held in various locations, including Seoul, where you might find temporary libraries or displays related to BTS.\\n3. **Public Libraries in Seoul** – If you're looking for a public library near BTS's headquarters or in the Gangnam area, the Seoul Metropolitan Library or Gangnam Library could be options.\\n\\nPlease specify if you mean a particular BTS-related location or a general library in your area, and I can give you more precise directions!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 64, 'total_tokens': 221, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C46WeXqGp7JCWGBRG3ioWAlUWvhBT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--300ea727-6ba6-4a2e-a0ec-4893929e92e9-0', usage_metadata={'input_tokens': 64, 'output_tokens': 157, 'total_tokens': 221, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e511772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Roger.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 281, 'total_tokens': 286, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C46Xvi4OTTtzafYfB46QRsLgFUBaR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d54d5301-8475-401d-a1a4-0e4639d61684-0', usage_metadata={'input_tokens': 281, 'output_tokens': 5, 'total_tokens': 286, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pipeline with history, getting a response using the same session ID\n",
    "pipeline_with_history.invoke( {\"query\": \"What's my name?\"}, config={\"session_id\": \"id_123\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dba16ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Roger. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 257, 'total_tokens': 269, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C46XkSRtVJ3iakSIk7v92b4UfiEVp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c1c917f3-71fb-4a1f-9308-60a63ccd9050-0', usage_metadata={'input_tokens': 257, 'output_tokens': 12, 'total_tokens': 269, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pipeline with history, getting a response using a new session ID\n",
    "pipeline_with_history.invoke({\"query\": \"What's my name?\"}, config={\"session_id\": \"id_456\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08552e38",
   "metadata": {},
   "source": [
    "We will use **PostgreSQL** for this example, with a database hosted on **NeonDB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20e4bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table (if it doesn't exist) to store chat history\n",
    "from langchain_postgres import PostgresChatMessageHistory\n",
    "import psycopg\n",
    "import os\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"POSTGRESQL_CONNECTION_STRING\")\n",
    "sync_connection = psycopg.connect(CONNECTION_STRING)\n",
    "\n",
    "table_name = \"chat_history\"\n",
    "PostgresChatMessageHistory.create_tables(sync_connection, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8eddfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing well, thank you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 144, 'total_tokens': 160, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C46a6IkqaGAMCFWZe5KoqNzkcJMWL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--64f1e9a5-e925-4eae-bc2e-5f516f925a22-0', usage_metadata={'input_tokens': 144, 'output_tokens': 16, 'total_tokens': 160, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "import uuid\n",
    "\n",
    "# Replace with your PostgreSQL credentials\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "# Initialize the chat history manager\n",
    "chat_history = PostgresChatMessageHistory(\n",
    "    table_name,\n",
    "    session_id,\n",
    "    sync_connection=sync_connection\n",
    ")\n",
    "\n",
    "# To simulate a conversation, we can add some initial messages to the chat history\n",
    "chat_history.add_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=\"Hello My name is Roger I live in Cajamarca, Peru, how are you?\"),\n",
    "        AIMessage(content=\"I'm doing well, thank you! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What is my name?\"),\n",
    "        AIMessage(content=\"Your name is Roger.\"),\n",
    "        HumanMessage(content=\"Where do I live?\"),\n",
    "        AIMessage(content=\"You live in Cajamarca, Peru.\"),\n",
    "        HumanMessage(content=\"How old am I?\"),\n",
    "        AIMessage(content=\"I'm not sure how old you are, but I can help you with other questions.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    lambda s: PostgresChatMessageHistory(table_name, s, sync_connection=sync_connection),\n",
    "    input_messages_key=\"query\",   # must match the prompt input variable\n",
    "    history_messages_key=\"history\"  # must match the prompt history variable\n",
    ")\n",
    "\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hello, how are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ee2d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f2841b21-cf50-43f8-8614-700d4aad4190'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03dfb60",
   "metadata": {},
   "source": [
    "We will use **DynamoDB** for this example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d62ebe",
   "metadata": {},
   "source": [
    "creating table in dynamodb if not exists\n",
    "> ⚠️ **Important:**  \n",
    "To create DynamoDB tables programmatically, your AWS user must have the necessary IAM permissions (e.g., `dynamodb:CreateTable`). Make sure your credentials allow table creation, otherwise you may encounter permission errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4045cd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table already exists, using existing table.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Get the service resource.\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "dynamo_table_name = \"ChatBotSessionTable\"\n",
    "try:\n",
    "    # Create the DynamoDB table.\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=dynamo_table_name,\n",
    "        KeySchema=[{\"AttributeName\": \"SessionId\", \"KeyType\": \"HASH\"}],\n",
    "        AttributeDefinitions=[{\"AttributeName\": \"SessionId\", \"AttributeType\": \"S\"}],\n",
    "        BillingMode=\"PAY_PER_REQUEST\",\n",
    "    )\n",
    "\n",
    "    # Wait until the table exists.\n",
    "    table.meta.client.get_waiter(\"table_exists\").wait(TableName=\"SessionTable\")\n",
    "\n",
    "    # Print out some data about the table.\n",
    "    print(table.item_count)\n",
    "except dynamodb.meta.client.exceptions.ResourceInUseException:\n",
    "    # If the table already exists, we can just get it\n",
    "    table = dynamodb.Table(dynamo_table_name)\n",
    "    print(\"Table already exists, using existing table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6062fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing well, thank you for asking. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 144, 'total_tokens': 162, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C46ems115ggpOy26li3LXsx7yJALL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d25221ed-a91a-411a-bc28-9c55bab809d2-0', usage_metadata={'input_tokens': 144, 'output_tokens': 18, 'total_tokens': 162, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "import uuid\n",
    "\n",
    "# Replace with your PostgreSQL credentials\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "chat_history = DynamoDBChatMessageHistory(table_name=dynamo_table_name, session_id=session_id)\n",
    "\n",
    "chat_history.add_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=\"Hello My name is Roger I live in Cajamarca, Peru, how are you?\"),\n",
    "        AIMessage(content=\"I'm doing well, thank you! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What is my name?\"),\n",
    "        AIMessage(content=\"Your name is Roger.\"),\n",
    "        HumanMessage(content=\"Where do I live?\"),\n",
    "        AIMessage(content=\"You live in Cajamarca, Peru.\"),\n",
    "        HumanMessage(content=\"How old am I?\"),\n",
    "        AIMessage(content=\"I'm not sure how old you are, but I can help you with other questions.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    lambda s: chat_history,\n",
    "    input_messages_key=\"query\",   # must match the prompt input variable\n",
    "    history_messages_key=\"history\"  # must match the prompt history variable\n",
    ")\n",
    "\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hello, how are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "832409bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Vives en Cajamarca, Perú.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 174, 'total_tokens': 182, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C46epMENoUxg1uPhrOt9GsWkG43Py', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--320a706e-f069-44a9-a982-6973b296d5a1-0', usage_metadata={'input_tokens': 174, 'output_tokens': 8, 'total_tokens': 182, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Donde vivo?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd82507",
   "metadata": {},
   "source": [
    "### 2. ConversationBufferWindowMemory\n",
    "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the **last k messages**. There are a few reasons why we would want to keep only the last k messages:\n",
    "\n",
    "⚡️ **Very Important**: More messages mean more tokens are sent with each request, and more tokens increases latency and cost. 💸⏳\n",
    "\n",
    "LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or \"forget\" information provided to them. Conciseness is key to high performing LLMs.\n",
    "\n",
    "If we keep all messages we will eventually hit the LLM's context window limit, by adding a window size k we can ensure we never hit this limit.\n",
    "\n",
    "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b973e876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is Roger\")\n",
    "memory.chat_memory.add_ai_message(\"Hey Roger, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10397b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is Roger', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey Roger, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is Roger', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey Roger, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is Roger.'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4dfa46",
   "metadata": {},
   "source": [
    "### 3. ConversationSummaryMemory\n",
    "Next up we have ConversationSummaryMemory, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
    "\n",
    "As before, we'll start with the original memory class before reimplementing it with the RunnableWithMessageHistory class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45d59023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bce4a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces themselves as James, and the AI greets him warmly, asking about his day and offering assistance or topics to discuss.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces themselves as James, and the AI greets him warmly, asking about his day and offering assistance or topics to discuss. James then mentions researching different types of conversational memory. The AI explains that conversational memory includes short-term and long-term types, describing their roles in context-aware responses and personalized interactions, and notes current research efforts to improve memory in AI systems.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human, James, mentions researching ConversationBufferMemory and ConversationBufferWindowMemory, to which the AI responds with explanations of both memory types, describing how ConversationBufferMemory stores the entire conversation history for coherence, while ConversationBufferWindowMemory retains only recent exchanges to manage context relevance and memory usage. The AI asks whether James is exploring these for a specific project or general understanding.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human, James, mentions researching ConversationBufferMemory and ConversationBufferWindowMemory, and the AI explains that ConversationBufferMemory stores the entire conversation history to maintain full context, aiding coherence in complex interactions, but it can increase memory usage; the AI asks if James is exploring these for a specific project or general understanding.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'The human, James, mentions researching ConversationBufferMemory and ConversationBufferWindowMemory, and the AI explains that ConversationBufferMemory stores the entire conversation history to maintain full context, aiding coherence in complex interactions, but it can increase memory usage; the AI asks if James is exploring these for a specific project or general understanding.',\n",
       " 'response': 'Exactly! ConversationBufferWindowMemory is designed to keep only the most recent k messages in memory, discarding older parts of the conversation as new messages arrive. This approach helps manage memory usage more efficiently, especially in long interactions, by limiting the amount of context the model needs to process at once. For example, if you set a window size of 5, then only the last 5 messages—be they user inputs or AI responses—are retained, while earlier parts are forgotten. This is particularly useful in scenarios where recent context is most relevant, but you want to avoid overloading the system with the entire conversation history. Are you considering using this for a specific application or just exploring different memory management options?'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18e113",
   "metadata": {},
   "source": [
    "# Summary Notes: Conversational Memory in LangChain\n",
    "\n",
    "This notebook explored the concept of conversational memory in LangChain, demonstrating how chatbots and agents can remember previous interactions to maintain context across conversations. Here’s a summary of the key concepts and implementations covered:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Conversational Memory Overview**\n",
    "- Conversational memory enables chatbots to reference and utilize previous messages, allowing for more natural and context-aware interactions.\n",
    "- Without memory, chatbots would only respond to the most recent message, losing all prior context.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **LangChain Memory Types**\n",
    "We reviewed several memory types available in LangChain (noting that some are being deprecated but are still useful for understanding):\n",
    "\n",
    "- **ConversationBufferMemory:** Stores the entire conversation history.\n",
    "- **ConversationBufferWindowMemory:** Stores only the last *k* messages, helping manage token usage and context window limits.\n",
    "- **ConversationSummaryMemory:** Maintains a summary of the conversation, rather than the full message history.\n",
    "- **ConversationSummaryBufferMemory:** Combines summary and buffer approaches for efficient memory management.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Implementing Memory with RunnableWithMessageHistory**\n",
    "- Introduced the `RunnableWithMessageHistory` class, the recommended approach for managing conversational memory in LangChain.\n",
    "- Demonstrated how to wrap a pipeline with message history, using session IDs to manage different conversations.\n",
    "- Showed how to define prompt templates and connect them to LLMs for context-aware responses.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Storing Memory in External Databases**\n",
    "- **PostgreSQL (NeonDB):** Demonstrated how to persist chat history in a PostgreSQL database using `PostgresChatMessageHistory`.\n",
    "- **DynamoDB:** Showed how to create and use a DynamoDB table for storing chat sessions with `DynamoDBChatMessageHistory`.\n",
    "- These approaches enable scalable, persistent memory across sessions and users.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Practical Examples**\n",
    "- Provided code examples for initializing and using each memory type.\n",
    "- Illustrated how to test memory by invoking pipelines and chains with different session IDs and queries.\n",
    "- Highlighted the importance of managing token usage and context window size for optimal LLM performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Key Takeaways**\n",
    "- **Memory is essential** for building effective conversational agents.\n",
    "- **Different memory types** serve different needs: full history, windowed history, or summarized context.\n",
    "- **External storage** (like PostgreSQL or DynamoDB) allows for persistent, scalable memory management.\n",
    "- **RunnableWithMessageHistory** is the modern, flexible way to integrate memory into LangChain workflows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3942b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
