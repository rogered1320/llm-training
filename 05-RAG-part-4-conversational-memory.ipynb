{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aef709d",
   "metadata": {},
   "source": [
    "# Conversational Memory for LangChain\n",
    "\n",
    "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
    "\n",
    "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058bc10",
   "metadata": {},
   "source": [
    "> âš ï¸ **Important**\n",
    "\n",
    "Although it is currently recommended to use the **LangGraph** library to implement chats with memory â€” since it allows you to efficiently work with complex workflows (for example: ðŸ—“ï¸ saving notes to Google Calendar, ðŸ—„ï¸ querying relational databases, and ðŸ¤– using MCPs) â€” in this notebook we will use **LangChain** to explain the basic concepts of conversational memory.\n",
    "\n",
    "This will make it easier to understand before moving on to more advanced tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30db397",
   "metadata": {},
   "source": [
    "## LangChain's Memory Types\n",
    "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
    "\n",
    "Throughout the notebook we will be referring to these older memory types and then rewriting them using the recommended RunnableWithMessageHistory class. We will learn about:\n",
    "\n",
    "- **ConversationBufferMemory:** the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
    "- **ConversationBufferWindowMemory:** similar to ConversationBufferMemory, but only keeps track of the last k messages.\n",
    "- **ConversationSummaryMemory:** rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
    "- **ConversationSummaryBufferMemory:** merges the ConversationSummaryMemory and ConversationTokenBufferMemory types.\n",
    "\n",
    "We'll work through each of these memory types in turn, and rewrite each one using the RunnableWithMessageHistory class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f685b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"llm-training-05-rag-p4\"\n",
    "\n",
    "llm =  ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.7,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71822e99",
   "metadata": {},
   "source": [
    "### 1. ConversationBufferMemory with RunnableWithMessageHistory\n",
    "When implementing `unnableWithMessageHistory` we will use LangChain Expression Language **(LCEL)** and for this we need to define our prompt template and LLM components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49f60f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n",
    "pipeline = prompt_template | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851b099",
   "metadata": {},
   "source": [
    "Our `RunnableWithMessageHistory` requires our pipeline to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a503cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f8f4f",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie history) and which to use for the user's query (ie query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3500eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a204af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Could you please specify which library you're referring to? Are you asking about a specific library at your location, a public library nearby, or a particular library related to your work at LangSmith? \\n\\nProviding a bit more detail will help me give you the most accurate information.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 187, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C1ZriMrpNW2hOOOBUgzYh7z1wQTFp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2c757058-92c3-4c53-b50d-407bd1fbbb8f-0', usage_metadata={'input_tokens': 187, 'output_tokens': 54, 'total_tokens': 241, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pipeline with history\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"where is the library?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5929a78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Roger', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello, Roger! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 26, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C1Zp33V2RwhnYr3rsXMREiICJEQN8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--519e9459-140f-4e0a-a144-7f3fb3d80e62-0', usage_metadata={'input_tokens': 26, 'output_tokens': 16, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content=\"What's my name\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Roger. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 53, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C1ZpGP7avyZRAP1zLuVSxlXbK4fkn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--063b9585-f71c-476b-9832-483b04c35bae-0', usage_metadata={'input_tokens': 53, 'output_tokens': 12, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='I work at LangSmith, what do I do there?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"As you work at LangSmith, you are likely involved in developing and managing language models or related AI technology. LangSmith is known for its work in AI, natural language processing, and machine learning. Your specific role could involve tasks such as training models, developing AI applications, or improving language understanding systems. \\n\\nIf you'd like, I can help you with more specific information or tasks related to your work at LangSmith. Just let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 85, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C1ZpwAr4bPICkLbzchbR9E2P8uafz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4011887f-73ae-40ed-88e4-aa7f37062a28-0', usage_metadata={'input_tokens': 85, 'output_tokens': 89, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e511772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Roger.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 54, 'total_tokens': 59, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C1YgKNHyddqquiwkwLkFMC9ZfJYmA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f30e3886-71d5-43c8-b07e-6a724d4fb35a-0', usage_metadata={'input_tokens': 54, 'output_tokens': 5, 'total_tokens': 59, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pipeline with history, getting a response using the same session ID\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What's my name?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba16ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I don't know your name unless you tell me. What's your name?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 24, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C1YgKXC5xHiUE0FR6quo4GsJGomB3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ce869a36-08c1-4931-a396-385d408b928a-0', usage_metadata={'input_tokens': 24, 'output_tokens': 16, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pipeline with history, getting a response using a new session ID\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What's my name?\"},\n",
    "    config={\"session_id\": \"id_456\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08552e38",
   "metadata": {},
   "source": [
    "We will use **PostgreSQL** for this example, with a database hosted on **NeonDB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e4bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table (if it doesn't exist) to store chat history\n",
    "from langchain_postgres import PostgresChatMessageHistory\n",
    "import psycopg\n",
    "import os\n",
    "\n",
    "CONNECTION_STRING = os.getenv(\"POSTGRESQL_CONNECTION_STRING\")\n",
    "sync_connection = psycopg.connect(CONNECTION_STRING)\n",
    "\n",
    "table_name = \"chat_history\"\n",
    "PostgresChatMessageHistory.create_tables(sync_connection, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8eddfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing well, thank you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 144, 'total_tokens': 160, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C1YmPzdceLc0IIyZEGi6WG9yxWQVi', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--63ffd87b-73c3-4c40-a82e-af82c4d130e9-0', usage_metadata={'input_tokens': 144, 'output_tokens': 16, 'total_tokens': 160, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "import uuid\n",
    "\n",
    "# Replace with your PostgreSQL credentials\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "# Initialize the chat history manager\n",
    "chat_history = PostgresChatMessageHistory(\n",
    "    table_name,\n",
    "    session_id,\n",
    "    sync_connection=sync_connection\n",
    ")\n",
    "\n",
    "chat_history.add_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=\"Hello My name is Roger I live in Cajamarca, Peru, how are you?\"),\n",
    "        AIMessage(content=\"I'm doing well, thank you! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What is my name?\"),\n",
    "        AIMessage(content=\"Your name is Roger.\"),\n",
    "        HumanMessage(content=\"Where do I live?\"),\n",
    "        AIMessage(content=\"You live in Cajamarca, Peru.\"),\n",
    "        HumanMessage(content=\"How old am I?\"),\n",
    "        AIMessage(content=\"I'm not sure how old you are, but I can help you with other questions.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    lambda s: PostgresChatMessageHistory(table_name, s, sync_connection=sync_connection),\n",
    "    input_messages_key=\"query\",   # must match the prompt input variable\n",
    "    history_messages_key=\"history\"  # must match the prompt history variable\n",
    ")\n",
    "\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hello, how are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd82507",
   "metadata": {},
   "source": [
    "### 2. ConversationBufferWindowMemory\n",
    "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the **last k messages**. There are a few reasons why we would want to keep only the last k messages:\n",
    "\n",
    "âš¡ï¸ **Very Important**: More messages mean more tokens are sent with each request, and more tokens increases latency and cost. ðŸ’¸â³\n",
    "\n",
    "LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or \"forget\" information provided to them. Conciseness is key to high performing LLMs.\n",
    "\n",
    "If we keep all messages we will eventually hit the LLM's context window limit, by adding a window size k we can ensure we never hit this limit.\n",
    "\n",
    "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b973e876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is Roger\")\n",
    "memory.chat_memory.add_ai_message(\"Hey Roger, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10397b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"I'm sorry, but I don't know your name since you haven't told me. Would you like to share it?\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4dfa46",
   "metadata": {},
   "source": [
    "### 3. ConversationSummaryMemory\n",
    "Next up we have ConversationSummaryMemory, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
    "\n",
    "As before, we'll start with the original memory class before reimplementing it with the RunnableWithMessageHistory class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45d59023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/84/tc7h8c5s2b54x2ck_2g9h4tc0000gn/T/ipykernel_8833/1366505295.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bce4a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James, and the AI responds warmly, greeting James and offering assistance.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human introduces himself as James, and the AI responds warmly, greeting James and offering assistance. When James mentions researching different types of conversational memory, the AI clarifies that conversational memory involves how both humans and AI systems retain and utilize information within a conversation. It explains that in AI, this includes short-term and long-term memory, and offers to explore specific types like working memory or episodic memory, showing readiness to assist further.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human, named James, mentions researching ConversationBufferMemory and ConversationBufferWindowMemory, and the AI explains that ConversationBufferMemory stores the full conversation history for context, while ConversationBufferWindowMemory maintains a sliding window of recent messages to optimize performance; the AI offers to provide more details or guidance on their use.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human, named James, mentions researching ConversationBufferMemory and ConversationBufferWindowMemory, and the AI explains that ConversationBufferMemory stores the full conversation history for context, while ConversationBufferWindowMemory maintains a sliding window of recent messages to optimize performance; the AI offers to provide more details or guidance on their use. The human clarifies that BufferMemory stores the entire conversation, and the AI confirms this, explaining the advantages and trade-offs of each approach, and offers to assist further.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'The human, named James, mentions researching ConversationBufferMemory and ConversationBufferWindowMemory, and the AI explains that ConversationBufferMemory stores the full conversation history for context, while ConversationBufferWindowMemory maintains a sliding window of recent messages to optimize performance; the AI offers to provide more details or guidance on their use. The human clarifies that BufferMemory stores the entire conversation, and the AI confirms this, explaining the advantages and trade-offs of each approach, and offers to assist further.',\n",
       " 'response': \"That's correct! ConversationBufferWindowMemory keeps a fixed number of the most recent messagesâ€”in other words, the last *k* messagesâ€”while discarding older ones beyond that window. This approach helps manage memory usage and maintains relevant context without overwhelming the system with the entire conversation history. It's especially useful when dealing with long interactions, as it ensures the AI focuses on the most recent exchanges, which are typically most relevant to the ongoing dialogue. If you'd like, I can help you determine the optimal window size or guide you on implementing this in your project!\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
